{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## TODO!!(점수 올리기 위해서)\n# 평가셋에서 점수가 잘 나오는 것으로 최적화\n# 불안정\n# 평가셋을 뽑을때도 제대로 뽑아야함<-테스트셋을 반영할 수 있게\n# 테스트셋 데이터의 분포를 반영\n# stratify<-classification분포에 맞게\n# regression문제는? classification처럼 구간을 나눔\n# 소금이 들어있는 비율로. (회귀문제를 분류모델처럼)\n# 정답칼럼을 기준으로\n\n# 직접 이미지를 가져올 때, size->101로, 모델 수정, 평가셋제대로 뽑기","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 소금의 양에 따라 stratify하는 코드\n# import zipfile\n\n# #with 키워드 쓰면, close안써도 알아서 닫힘<-코드 줄 수 아낌\n# with zipfile.ZipFile('/kaggle/input/tgs-salt-identification-challenge/train.zip','r') as z:\n#     z.extractall('train') #train(.zip)폴더의 압축을 푼다.\n\n# with zipfile.ZipFile('/kaggle/input/tgs-salt-identification-challenge/test.zip','r') as z :\n#     z.extractall('test') #test(.zip)폴더의 압축을 푼다.\n\n# from tqdm import tqdm_notebook\n\n# #keras.preprocessing.image load_img로 이미지 확인\n# #keras.preprocessing.image load_img와 PIL Image open은 같은 결과\n# from keras.preprocessing.image import load_img, img_to_array\n# from skimage.transform import resize\n# import matplotlib.pyplot as plt\n\n\n# train = pd.read_csv('/kaggle/input/tgs-salt-identification-challenge/train.csv')\n\n# train.head()\n\n# train['img'] = [np.array(load_img('train/images/'+x+'.png',grayscale=True))/255 for x in tqdm_notebook(train['id'])] # total_id <-이미지 불러오고 몇장인지 모를떄만 사용\n\n# train['mask'] = [np.array(load_img('train/masks/'+x+'.png',grayscale=True))/255 for x in tqdm_notebook(train['id'])]\n\n# train\n\n# train[train['id']=='75efad62c1']['mask'].sum().sum()\n\n# train['coverage'] =train['mask'].apply(np.sum) / 10201\n\n# train['class']=(train['coverage']*10).astype(int)\n\n# train['class']\n\n# train['img'] #2차원\n# reshape_img = np.asarray(train['img'].tolist()).reshape(-1,101,101,1)\n\n# reshape_mask = np.asarray(train['mask'].tolist()).reshape(-1,101,101,1)\n\n# reshape_mask.shape\n\n\n# from sklearn.model_selection import train_test_split \n# x_train,x_valid,y_train,y_valid = train_test_split(reshape_img,reshape_mask,test_size=0.2,random_state=777,stratify=train['class']) #stratify의 분포에 맞게 split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import zipfile\n\n#with 키워드 쓰면, close안써도 알아서 닫힘<-코드 줄 수 아낌\nwith zipfile.ZipFile('/kaggle/input/tgs-salt-identification-challenge/train.zip','r') as z:\n    z.extractall('train') #train(.zip)폴더의 압축을 푼다.\n\nwith zipfile.ZipFile('/kaggle/input/tgs-salt-identification-challenge/test.zip','r') as z :\n    z.extractall('test') #test(.zip)폴더의 압축을 푼다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_id = os.listdir('train/images')\ntest_id = os.listdir('test/images')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PIL로 이미지 확인\nos.listdir('train')\n\nos.listdir('train/masks')\n\nfrom PIL import Image\nx=Image.open('train/masks/182bfc6862.png')\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nx.size\n\nos.listdir('train/images/')\n\nx=Image.open('train/images/182bfc6862.png')\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#keras.preprocessing.image load_img로 이미지 확인\n#keras.preprocessing.image load_img와 PIL Image open은 같은 결과\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\n\nplt.imshow(load_img('train/images/182bfc6862.png'))\n\n\nplt.imshow(load_img('train/masks/182bfc6862.png'))\n\n#흑백, 1차원\nx=load_img('train/masks/182bfc6862.png')\nx.size\n\n#흑백, 1차원\nx=load_img('train/images/182bfc6862.png')\nx.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# /////////////////////// #","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = Image.open('train/images/182bfc6862.png')\nimg = np.asarray(img)\nimg.shape\n\nimg # 3차원에 다 같은 값 들어있네->흑백이다.\n\nimg = img[:,:,0]\n\nimg2= Image.fromarray(img)\nimg2\n\n\n\nmask = Image.open('train/masks/182bfc6862.png')\nmask = np.asarray(img) #2차원이네\n\n\nmask = Image.open('train/masks/89165071a4.png')\nmask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # 이미지를 nparray로 가져왔다->이제부터는 nparray로 처리함 cf)그래서 sklearn가지고 resize함수 이용해도 상관 없는 거\n# # 이미지 전처리, resize\n# from tqdm import tqdm_notebook\n# X_train = np.zeros((len(train_id),128,128,1),dtype=np.uint8)\n# y_train = np.zeros((len(train_id),128,128,1),dtype=np.bool)\n# for i , file_name in tqdm_notebook(enumerate(train_id),total=len(train_id)):\n#     img = Image.open('train/images/'+file_name)\n#     img = np.asarray(img)[:,:,0]\n#     X_train[i] = resize(img,(128,128,1),mode = 'constant',preserve_range=True)\n    \n#     mask = Image.open('train/masks/'+file_name)\n#     mask = np.asarray(img)[:,:]\n#     y_train[i] = resize(img,(128,128,1),mode='constant',preserve_range=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook\nX_train = np.zeros((len(train_id), 128, 128, 1), dtype=np.uint8)\ny_train = np.zeros((len(train_id), 128, 128, 1), dtype=np.bool)\nfor i, file_name in tqdm_notebook(enumerate(train_id), total=len(train_id)):\n    img = load_img('train/images/' + file_name)\n    img = img_to_array(img)[:,:,0]\n    X_train[i] = resize(img, (128, 128, 1), mode='constant', preserve_range=True)\n    \n    mask = load_img('train/masks/' + file_name)\n    mask = img_to_array(mask)[:,:,0]\n    y_train[i] = resize(mask, (128, 128, 1), mode='constant', preserve_range=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#계속 image->np array로 바꾼 상태에서 처리중\n#sklearn(머신러닝 툴킷, np array 처리 쉽게 가능)계속 쓸 거임\n#train,val나누기 8:2 비율로\n\nfrom sklearn.model_selection import train_test_split \nx_train,x_valid,y_train,y_valid = train_test_split(X_train,y_train,test_size=0.2,random_state=777)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#이미지 좌우반전\n#np.fliplr() left right 반전(축 안 써줘도 알아서 잘 해줌)\nx_train = np.append(x_train,[np.fliplr(x) for x in x_train],axis=0) # 행 기준으로 행 아래에 append해줌\ny_train = np.append(y_train,[np.fliplr(y) for y in y_train],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convolution_block(x,filters,size,strides=(1,1),padding='same',activation=True):\n    x = Conv2D(filters,size,strides=strides,padding=padding)(x)\n    x = BatchNormalization()(x)\n    if activation==True :\n        x = Activation('relu')(x)\n    return x\n\ndef residual_block(block_input,num_filters=16):\n    x = Activation('relu')(block_input)\n    x = BatchNormalization()(x)\n    x = convolution_block(x,num_filters,(3,3))\n    x = convolution_block(x,num_filters,(3,3),activation=False) # residual block 맨 마지막엔 activation x\n    x = Add()([x,block_input]) # keras.layers에 있는 Add\n    #x = x+block_input\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model (input_layer,start_neuron,dropout_ratio=0.5):\n    cnn1 = Conv2D(start_neuron*1,(3,3),padding='same',activation=None)(input_layer)\n    cnn1 = residual_block(cnn1,start_neuron*1)\n    cnn1 = residual_block(cnn1,start_neuron*1)\n    cnn1 = Activation('relu')(cnn1)\n    pool1 = MaxPool2D()(cnn1)\n    pool1 = Dropout(dropout_ratio/2)(pool1)\n    \n    cnn2 = Conv2D(start_neuron*2,(3,3),padding='same',activation=None)(pool1)\n    cnn2 = residual_block(cnn2,start_neuron*2)\n    cnn2 = residual_block(cnn2,start_neuron*2)\n    cnn2 = Activation('relu')(cnn2)\n    pool2 = MaxPool2D()(cnn2)\n    pool2 = Dropout(dropout_ratio)(pool2)\n    \n    cnn3 = Conv2D(start_neuron*4,(3,3),padding='same',activation=None)(pool2)\n    cnn3 = residual_block(cnn3,start_neuron*4)\n    cnn3 = residual_block(cnn3,start_neuron*4)\n    cnn3 = Activation('relu')(cnn3)\n    pool3 = MaxPool2D()(cnn3)\n    pool3 = Dropout(dropout_ratio)(pool3)\n    \n    cnn4 = Conv2D(start_neuron*8,(3,3),padding='same',activation=None)(pool3)\n    cnn4 = residual_block(cnn4,start_neuron*8)\n    cnn4 = residual_block(cnn4,start_neuron*8)\n    cnn4 = Activation('relu')(cnn4)\n    pool4 = MaxPool2D()(cnn4)\n    pool4 = Dropout(dropout_ratio)(pool4)\n    \n    cnnm = Conv2D(start_neuron*16,(3,3),padding='same',activation=None)(pool4)\n    cnnm = residual_block(cnnm,start_neuron*16)\n    cnnm = residual_block(cnnm,start_neuron*16)\n    cnnm = Activation('relu')(cnnm)\n    \n    #block6 out shape:()\n    cc4 = Conv2DTranspose(start_neuron*8,(3,3),padding='same',strides=(2,2))(cnnm) #Conv2DTrans=deconvolution(풀링하고,convolution하는 것). 여기서는 2배로 풀링한 후, 그것을 3x3필터로 convolution한다, 패딩을 했기 때문에 최종적으로는 크기가 2배가 된다.\n    cc4 = concatenate([cc4,cnn4])\n    cc4 = Dropout(dropout_ratio)(cc4)\n    cnn4 = Conv2D(start_neuron*8,(3,3),activation=None,padding='same')(cc4)\n    cnn4 = residual_block(cnn4,start_neuron*8)\n    cnn4 = residual_block(cnn4,start_neuron*8)\n    cnn4 = Activation('relu')(cnn4)\n    \n    #block6 out shape:()\n    cc3 = Conv2DTranspose(start_neuron*4,(3,3),padding='same',strides=(2,2))(cnn4) #Conv2DTrans=deconvolution(풀링하고,convolution하는 것). 여기서는 2배로 풀링한 후, 그것을 3x3필터로 convolution한다, 패딩을 했기 때문에 최종적으로는 크기가 2배가 된다.\n    cc3 = concatenate([cc3,cnn3])\n    cc3 = Dropout(dropout_ratio)(cc3)\n    cnn3 = Conv2D(start_neuron*4,(3,3),activation=None,padding='same')(cc3)\n    cnn3 = residual_block(cnn3,start_neuron*4)\n    cnn3 = residual_block(cnn3,start_neuron*4)\n    cnn3 = Activation('relu')(cnn3)\n    \n    #block6 out shape:()\n    cc2 = Conv2DTranspose(start_neuron*2,(3,3),padding='same',strides=(2,2))(cnn3) #Conv2DTrans=deconvolution(풀링하고,convolution하는 것). 여기서는 2배로 풀링한 후, 그것을 3x3필터로 convolution한다, 패딩을 했기 때문에 최종적으로는 크기가 2배가 된다.\n    cc2 = concatenate([cc2,cnn2])\n    cc2 = Dropout(dropout_ratio)(cc2)\n    cnn2 = Conv2D(start_neuron*2,(3,3),activation=None,padding='same')(cc2)\n    cnn2 = residual_block(cnn2,start_neuron*2)\n    cnn2 = residual_block(cnn2,start_neuron*2)\n    cnn2 = Activation('relu')(cnn2)\n    \n    #block6 out shape:()\n    cc1 = Conv2DTranspose(start_neuron*1,(3,3),padding='same',strides=(2,2))(cnn2) #Conv2DTrans=deconvolution(풀링하고,convolution하는 것). 여기서는 2배로 풀링한 후, 그것을 3x3필터로 convolution한다, 패딩을 했기 때문에 최종적으로는 크기가 2배가 된다.\n    cc1 = concatenate([cc1,cnn1])\n    cc1 = Dropout(dropout_ratio)(cc1)\n    cnn1 = Conv2D(start_neuron*1,(3,3),activation=None,padding='same')(cc1)\n    cnn1 = residual_block(cnn1,start_neuron*1)\n    cnn1 = residual_block(cnn1,start_neuron*1)\n    cnn1 = Activation('relu')(cnn1)\n    cnn1 = Dropout(dropout_ratio/2)(cnn1)\n    \n    output_layer = Conv2D(1,(1,1),activation='sigmoid')(cnn1)\n    return output_layer\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input((128,128,1))\noutput_layer = build_model (input_layer,16,0.5)\nmodel = Model(input_layer,output_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # 모델링. U-net(핵심 : U자모양에서 대응되는 반대편을 채널의 뒤에 이어붙임)\n# from keras.models import Model\n# from keras.layers import *\n\n# #block1 out shape:(64,64,8)\n# inp = Input((128,128,1))\n# cnn1 = Conv2D(8,(3,3),activation='relu',padding='same')(inp)\n# cnn1 = Conv2D(8,(3,3),activation='relu',padding='same')(cnn1)\n# pool1 = MaxPooling2D()(cnn1)\n\n# #block2 out shape:(32,32,16)\n# cnn2 = Conv2D(16,(3,3),activation='relu',padding='same')(pool1)\n# cnn2 = Conv2D(16,(3,3),activation='relu',padding='same')(cnn2)\n# pool2 = MaxPooling2D()(cnn2)\n\n# #block3 out shape:(16,16,32)\n# cnn3 = Conv2D(32,(3,3),activation='relu',padding='same')(pool2)\n# cnn3 = Conv2D(32,(3,3),activation='relu',padding='same')(cnn3)\n# pool3 = MaxPooling2D()(cnn3)\n\n# #block4 out shape:(8,8,64)\n# cnn4 = Conv2D(64,(3,3),activation='relu',padding='same')(pool3)\n# cnn4 = Conv2D(64,(3,3),activation='relu',padding='same')(cnn4)\n# pool4 = MaxPooling2D()(cnn4)\n\n# #block5 out shape:(8,8,128)\n# cnn5 = Conv2D(128,(3,3),activation='relu',padding='same')(pool4)\n# cnn5 = Conv2D(128,(3,3),activation='relu',padding='same')(cnn5)\n\n# #block6 out shape:()\n# cc4 = Conv2DTranspose(64,(3,3),padding='same',strides=(2,2))(cnn5) #Conv2DTrans=deconvolution(풀링하고,convolution하는 것). 여기서는 2배로 풀링한 후, 그것을 3x3필터로 convolution한다, 패딩을 했기 때문에 최종적으로는 크기가 2배가 된다.\n# cc4 = concatenate([cc4,cnn4])\n# cnn4 = Conv2D(64,(3,3),activation='relu',padding='same')(cc4)\n# cnn4 = Conv2D(64,(3,3),activation='relu',padding='same')(cnn4)\n\n# #block7 out shape:()\n# cc3 = Conv2DTranspose(32,(3,3),padding='same',strides=(2,2))(cnn4)\n# cc3 = concatenate([cc3,cnn3])\n# cnn3 = Conv2D(32,(3,3),activation='relu',padding='same')(cc3)\n# cnn3 = Conv2D(32,(3,3),activation='relu',padding='same')(cnn3)\n\n# #block8 out shape:()\n# cc2 = Conv2DTranspose(16,(3,3),padding='same',strides=(2,2))(cnn3)\n# cc2 = concatenate([cc2,cnn2])\n# cnn2 = Conv2D(16,(3,3),activation='relu',padding='same')(cc2)\n# cnn2 = Conv2D(16,(3,3),activation='relu',padding='same')(cnn2)\n\n# #block9 out shape:()\n# cc1 = Conv2DTranspose(8,(3,3),padding='same',strides=(2,2))(cnn2)\n# cc1 = concatenate([cc1,cnn1])\n# cnn1 = Conv2D(8,(3,3),activation='relu',padding='same')(cc1)\n# cnn1 = Conv2D(8,(3,3),activation='relu',padding='same')(cnn1)\n\n# out = Conv2D(1,(1,1),activation='sigmoid')(cnn1)\n# model = Model(inp,out)\n\n# model.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\ncallbacks=[ModelCheckpoint(\"best.h5\",save_best_only=True,verbose=1),EarlyStopping(patience=15),ReduceLROnPlateau(patience=8,factor=0.15,verbose=1)]\n\nmodel.fit(x_train,y_train,validation_data=[x_valid,y_valid],batch_size=32,callbacks=callbacks,epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook\nX_test = np.zeros((len(test_id),128,128,1),dtype=np.uint8)\nfor i, filename in tqdm_notebook(enumerate(test_id),total=len(test_id)):\n    img = Image.open('test/images/'+filename)\n    img = np.asarray(img)[:,:,0]\n    X_test[i] = resize(img,(128,128,1),mode='constant',preserve_range=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(\"best.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_valid = model.predict(x_valid).reshape(-1,128,128)\ny_valid = y_valid.reshape(-1,128,128)\n\ndef down(image): #이미지 원래 크기로 되돌림\n    return resize(image,(101,101),mode='constant',preserve_range=True)\n\npreds_valid = np.array([down(x) for x in preds_valid])\ny_valid = np.array([down(y) for y in y_valid])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n#     print(\"intersection type:\",type(intersection))\n#     print(\"intersection shape:\",intersection.shape)\n#     print(intersection)\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n#     print(\"area_true type:\",type(area_true))\n#     print(\"area_true shape:\",area_true.shape)\n#     print(area_true)\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n#     print(\"area_pred type:\",type(area_pred))\n#     print(\"area_pred shape:\",area_pred.shape)\n#     print(area_pred)\n    area_true = np.expand_dims(area_true, -1) # axis=1(열)로 차원 확장 -> (2,)->(2,1)\n#     print(\"expand area_true type:\",type(area_true))\n#     print(\"expand area_true shape:\",area_true.shape)\n#     print(area_true)\n    area_pred = np.expand_dims(area_pred, 0) # axis =0(행)로 차원 호가장 ->(2,)->(1,2)\n#     print(\"expandarea_pred type:\",type(area_pred))\n#     print(\"expand area_pred shape:\",area_pred.shape)\n#     print(area_pred)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n    #차원이 다 다른데 어떻게 연산을 하는 거지\n#     print(\"union type:\",type(union))\n#     print(\"union shape:\",union.shape)\n#     print(union)\n\n    # Exclude background from the analysis\n#     print(\"-------------\")\n    intersection = intersection[1:,1:]\n#     print(\"intersection type:\",type(intersection))\n#     print(\"intersection shape:\",intersection.shape)\n#     print(intersection)\n    union = union[1:,1:]\n#     print(\"union type:\",type(union))\n#     print(\"union shape:\",union.shape)\n#     print(union)\n    union[union == 0] = 1e-9\n#     print(\"union[union==0] type:\",type(union))\n#     print(\"union[union==0] shape:\",union.shape)\n#     print(union)\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n        \n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = np.linspace(0,1,50) # 0~1을 50으로 나누어줌\nious = np.array([iou_metric_batch(y_valid, np.int32(preds_valid>x)) for x in tqdm_notebook(threshold)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold_best_index = np.argmax(ious)\nthreshold_best_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iou_best = ious[threshold_best_index]\niou_best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold_best = threshold[threshold_best_index]\nthreshold_best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,12))\nplt.plot(threshold, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"best threshold\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.predict(X_test, batch_size=128, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import trange\ndown_test = []\nfor i in trange(len(result)):\n    down_test.append(resize(np.squeeze(result[i]), (101, 101), mode='constant', preserve_range=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle_encode(im):\n    '''\n    im: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = im.flatten(order = 'F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nresult_dic = {j:rle_encode(np.round(down_test[i]>=threshold_best)) for i, j in enumerate(tqdm_notebook(os.listdir('test/images')))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(result_dic, orient='index')\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.index.names = ['id']\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.columns = ['rle_mask']\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sub.reset_index()\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['id'] = sub['id'].apply(lambda x: x[:-4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}